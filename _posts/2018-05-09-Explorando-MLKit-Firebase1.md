---
layout: post
title:  "Explorando o ML Kit for Firebase"
date:   2018-05-09 05:00:00 +0200
categories: firebase
---
O artigo original pode ser encontrado no [Medium](https://medium.com/android-dev-moz/mlkit-540f8e5438c1).
<section name="298b" class="section section--body section--first"><h4 name="6983" id="6983" class="graf graf--h4 graf-after--h3 graf--subtitle">Parte 1 - O novo serviÃ§o de Machine Learning doÂ Firebase</h4><p name="c42c" id="c42c" class="graf graf--p graf-after--h4">Este artigo faz parte da sÃ©rie Explorando o ML Kit for Firebase:</p><ol class="postList"><li name="e33c" id="e33c" class="graf graf--li graf-after--p">O novo serviÃ§o de Machine Learning do Firebase</li><li name="c66f" id="c66f" class="graf graf--li graf-after--li graf--trailing"><a href="/firebase/2019/02/01/Explorando-MLKit-Firebase2.html" data-href="/firebase/2019/02/01/Explorando-MLKit-Firebase2.html" class="markup--anchor markup--li-anchor" target="_blank">Linguagem Natural e IdentificaÃ§Ã£o de Linguagem</a></li></ol></section><section name="3bbb" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="881d" id="881d" class="graf graf--p graf--leading">Durante a sessÃ£o de abertura do <a href="https://events.google.com/io" data-href="https://events.google.com/io" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Google I/O 2018</a>, que aconteceu ontem(8 de Maio), a Google apresentou algumas novidades para o Firebase: melhorias no <a href="https://firebase.google.com/docs/ab-testing/" data-href="https://firebase.google.com/docs/ab-testing/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">A/B Testing</a>, suporte para <a href="https://firebase.google.com/docs/test-lab/ios/firebase-console" data-href="https://firebase.google.com/docs/test-lab/ios/firebase-console" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">iOS no Test Lab</a>, a versÃ£o estÃ¡vel do <a href="https://firebase.google.com/docs/perf-mon/" data-href="https://firebase.google.com/docs/perf-mon/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Performance Monitor</a> e a versÃ£o <em class="markup--em markup--p-em">preview</em> do <a href="https://firebase.google.com/docs/ml-kit/" data-href="https://firebase.google.com/docs/ml-kit/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ML Kit for Firebase</a>.</p><p name="b57a" id="b57a" class="graf graf--p graf-after--p">De acordo com a documentaÃ§Ã£o, o <a href="https://firebase.google.com/products/ml-kit/" data-href="https://firebase.google.com/products/ml-kit/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ML Kit</a> for Firebase Ã©:</p><blockquote name="bebb" id="bebb" class="graf graf--blockquote graf-after--p">um SDK para dispositivos mÃ³veis que leva Machine Learning da Google para aplicativos Android e iOS em um pacote poderoso e fÃ¡cil de usar.</blockquote><p name="80f5" id="80f5" class="graf graf--p graf-after--blockquote">VocÃª provavelmente jÃ¡ esteve numa situaÃ§Ã£o em que precisava de implementar alguma forma de InteligÃªncia Artificial ou Machine Learning na sua aplicaÃ§Ã£o para tornar ela mais â€œinteligenteâ€ e melhorar a experiÃªncia do seu utilizador. Nessa situaÃ§Ã£o, vocÃª provavelmente nÃ£o conseguiu comeÃ§ar a utilizar Machine Learning por ser complicado de aprender, ligeiramente mais caro ou talvez porque vocÃª nÃ£o queria hospedar um modelo ML em um outro serviÃ§o.<br>O ML Kit foi introduzido para resolver este problema. Com o ML Kit, vocÃª pode utilizar serviÃ§os de Machine Learning com a mesma simplicidade do Firebase.</p><p name="1ff0" id="1ff0" class="graf graf--p graf-after--p">ML Kit for Firebase traz-nos funcionalidades como:</p><ul class="postList"><li name="5bfe" id="5bfe" class="graf graf--li graf-after--p">Legendas para imagens;</li><li name="cf0d" id="cf0d" class="graf graf--li graf-after--li">Reconhecimento de texto;</li><li name="4a1b" id="4a1b" class="graf graf--li graf-after--li">DeteÃ§Ã£o facial;</li><li name="7aa1" id="7aa1" class="graf graf--li graf-after--li">Scan de cÃ³digo de barras;</li><li name="246a" id="246a" class="graf graf--li graf-after--li">DeteÃ§Ã£o de pontos de referÃªncia;</li><li name="1866" id="1866" class="graf graf--li graf-after--li">E respostas inteligentes? ğŸ¤”</li></ul><p name="ab92" id="ab92" class="graf graf--p graf-after--li">Fiquei bastante entusiasmado quando eles apresentaram esta funcionalidade que combina <a href="https://cloud.google.com/vision/?authuser=0" data-href="https://cloud.google.com/vision/?authuser=0" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Google Cloud Vision API</a>, <a href="https://www.tensorflow.org/mobile/tflite/?authuser=0" data-href="https://www.tensorflow.org/mobile/tflite/?authuser=0" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">TensorFlow Lite</a>, e <a href="https://developer.android.com/ndk/guides/neuralnetworks/?authuser=0" data-href="https://developer.android.com/ndk/guides/neuralnetworks/?authuser=0" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Android Neural Networks API</a>. Por isso, decidi experimentar na hora e partilhar a minha experiÃªncia.</p><h3 name="68ab" id="68ab" class="graf graf--h3 graf-after--p">On-device vs.Â Cloud</h3><p name="22a4" id="22a4" class="graf graf--p graf-after--h3">Antes de comeÃ§ar a falar sobre as tecnologias, gostaria de explicar a diferenÃ§a entre on-device e cloud.</p><ul class="postList"><li name="192b" id="192b" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">on-device</strong> significa que a funcionalidade estÃ¡ disponÃ­vel no dispositivo, e nÃ£o precisa de uma conexÃ£o Ã  internet para funcionar. No ML Kit, estas funcionalidades provavelmente sÃ£o fornecidas atravÃ©s do TensorFlow Lite e Android Neural Networks API.<br>As tecnologias on-device do ML Kit sÃ£o: Reconhecimento de Texto, deteÃ§Ã£o facial, scan de cÃ³digo de barras e legendas para imagens.</li><li name="7df1" id="7df1" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">cloud</strong> significa que a funcionalidade sÃ³ estÃ¡ disponÃ­vel na cloud. O que significa que o seu dispositivo mÃ³vel farÃ¡ uma requisiÃ§Ã£o Ã  cloud e tem de esperar por uma resposta (atravÃ©s da internet, obviamente). O ML Kit utiliza a Cloud Vision API para estas funcionalidades. Ã‰ importante notar que esta API precisa que vocÃª registre uma forma de pagamento para poder usÃ¡-la. No Firebase, isto significa fazer upgrade para o plano Blaze.<br>As tecnologias cloud do ML Kit sÃ£o: Reconhecimento de Texto, Legendas para imagens e DetecÃ§Ã£o de pontos de referÃªncia.</li></ul><p name="758b" id="758b" class="graf graf--p graf-after--li">Se vocÃª estiver no plano Sparkle do Firebase, vocÃª sÃ³ tem acesso Ã s funcionalidades on-device do ML Kit, como mostra a <a href="https://firebase.google.com/pricing/?hl=pt-pt" data-href="https://firebase.google.com/pricing/?hl=pt-pt" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">tabela de preÃ§os do Firebase</a>. Para utilizar as funcionalidades cloud, vocÃª terÃ¡ de passar para o plano Blaze, como mencionei anteriormente.<br>Neste artigo irei demonstrar apenas as funcionalidades on-device.</p><p name="b2d6" id="b2d6" class="graf graf--p graf-after--p graf--trailing">Vou lembrar-lhe que esta funcionalidade ainda estÃ¡ na versÃ£o <em class="markup--em markup--p-em">â€œpreviewâ€</em>, o que significa que sÃ³ pode ser utilizada para testes. <strong class="markup--strong markup--p-strong">NÃ£o utilize em aplicaÃ§Ãµes que se encontram em produÃ§Ã£o.</strong></p></div></div></section><section name="7bdc" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="65d6" id="65d6" class="graf graf--h3 graf--leading">ML Vision</h3><p name="7b86" id="7b86" class="graf graf--p graf-after--h3">Vision Ã© o novo SDK do Firebase que permite que vocÃª use os pacotes de inteligÃªncia artificial que fazem o uso da cÃ¢mera do dispositivo. Todas as funcionalidades que mencionei acima estÃ£o inclusas neste pacote.</p><p name="07d8" id="07d8" class="graf graf--p graf-after--p">Para poder ter estas funcionalidades na sua app, vocÃª precisa de:</p><p name="981f" id="981f" class="graf graf--p graf-after--p">1. <a href="https://firebase.google.com/docs/android/setup?hl=pt-pt" data-href="https://firebase.google.com/docs/android/setup?hl=pt-pt" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Adicionar o Firebase Ã  app</a> e de seguida adicionar a dependÃªncia do ML Vision:</p><pre name="2b4d" id="2b4d" class="graf graf--pre graf-after--p">implementation &#39;com.google.firebase:firebase-ml-vision:15.0.0&#39;</pre><p name="8b99" id="8b99" class="graf graf--p graf-after--pre">2. (opcional) Adicionar o modelo ML Ã  sua app. Basta colocar a seguinte linha ao AndroidManifest.xml:</p><pre name="315a" id="315a" class="graf graf--pre graf-after--p">&lt;meta-data<br>    android:name=&quot;com.google.firebase.ml.vision.DEPENDENCIES&quot;<br>    android:value=&quot;text&quot; /&gt;</pre><p name="4c8d" id="4c8d" class="graf graf--p graf-after--pre">Isto farÃ¡ com que a app baixe o modelo ML quando for instalada pela Google Playstore.</p><p name="6c3d" id="6c3d" class="graf graf--p graf-after--p">O valor â€œtextâ€ serve para a funcionalidade de reconhecimento de texto. Se vocÃª quiser adicionar outra funcionalidade, pode colocÃ¡-las separadas por vÃ­rgula, por exemplo:</p><pre name="531f" id="531f" class="graf graf--pre graf-after--p">android:value=&quot;text,barcode,face,label&quot;</pre><p name="78c4" id="78c4" class="graf graf--p graf-after--pre">3. Criar uma variÃ¡vel do tipo <code class="markup--code markup--p-code">FirebaseVisionImage</code>. Para instanciÃ¡-la, vocÃª precisa passar por parÃ¢metro um <code class="markup--code markup--p-code">Bitmap</code>, <code class="markup--code markup--p-code">Image</code>, <code class="markup--code markup--p-code">ByteBuffer</code> (ou array de <code class="markup--code markup--p-code">byte</code>) ou um <code class="markup--code markup--p-code">Uri</code> que contem o caminho da imagem.<br>Note que independentemente do parÃ¢metro que vocÃª passa, Ã© importante que a imagem esteja â€œde pÃ©â€, ou seja na rotaÃ§Ã£o correta, senÃ£o o Firebase nÃ£o consegue reconhece-la.</p><p name="cc74" id="cc74" class="graf graf--p graf-after--p">Para experimentar o ML Kit eu instanciei a classe <code class="markup--code markup--p-code">FirebaseVisionImage</code> com uma <code class="markup--code markup--p-code">Uri</code>. Estou a utilizar imagens da galeria por ser a forma mais simples de usar o ML Kit.</p><h4 name="5a14" id="5a14" class="graf graf--h4 graf-after--p">Reconhecimento deÂ Texto</h4><p name="f371" id="f371" class="graf graf--p graf-after--h4">Esta funcionalidade permite que o dispositivo do utilizador leia texto (em um idioma baseado no latim) em tempo real, atravÃ©s da cÃ¢mara.</p><p name="55c5" id="55c5" class="graf graf--p graf-after--p">Para utilizar o Reconhecimento de Texto, vocÃª precisa de utilizar o <code class="markup--code markup--p-code">FirebaseVisionTextDetector</code>:</p><pre name="477e" id="477e" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">FirebaseVisionTextDetector detector = FirebaseVision.getInstance()<br>    .getVisionTextDetector();</code></pre><p name="709a" id="709a" class="graf graf--p graf-after--pre">De seguida, chame o mÃ©todo <code class="markup--code markup--p-code">detectInImage()</code>, passando a nossa FirebaseVisionImage e adicione um listener. Nesse listener vocÃª poderÃ¡ obter os blocos de texto que foram lidos da imagem. Veja o cÃ³digo completo no fim do artigo.</p><h4 name="d2de" id="d2de" class="graf graf--h4 graf-after--p">Scan de cÃ³digo deÂ barras</h4><p name="12e0" id="12e0" class="graf graf--p graf-after--h4">Tal como o nome indica, esta funcionalidade permite ler um cÃ³digo de barras. Para tal, vocÃª precisa especificar os formatos de cÃ³digo de barras que a sua aplicaÃ§Ã£o pode ler(veja a <a href="https://firebase.google.com/docs/ml-kit/android/read-barcodes?hl=pt-pt#configure-the-barcode-detector" data-href="https://firebase.google.com/docs/ml-kit/android/read-barcodes?hl=pt-pt#configure-the-barcode-detector" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">lista completa de formatos aqui</a>), atravÃ©s de um objecto <code class="markup--code markup--p-code">FirebaseVisionBarcodeDetectorOptions</code>:</p><pre name="d474" id="d474" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">FirebaseVisionBarcodeDetectorOptions options =<br>    new FirebaseVisionBarcodeDetectorOptions.Builder()<br>        .setBarcodeFormats(FirebaseVisionBarcode.FORMAT_QR_CODE,<br>                           FirebaseVisionBarcode.FORMAT_AZTEC)<br>        .build();</code></pre><p name="e9fe" id="e9fe" class="graf graf--p graf-after--pre">De seguida, o processo Ã© o mesmo: usar o Detector, adicionar um listener e receber o cÃ³digo de barras.</p><pre name="5383" id="5383" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">FirebaseVisionBarcodeDetector detector = FirebaseVision.getInstance()<br>    .getVisionBarcodeDetector(options);</code></pre><p name="0c5a" id="0c5a" class="graf graf--p graf-after--pre">Encontre o cÃ³digo completo no fim do artigo.</p><h4 name="cfe9" id="cfe9" class="graf graf--h4 graf-after--p">DeteÃ§Ã£o Facial</h4><p name="2703" id="2703" class="graf graf--p graf-after--h4">A deteÃ§Ã£o facial Ã© capaz de encontrar faces humanas numa imagem e identificar caracterÃ­sticas como:</p><ul class="postList"><li name="33ed" id="33ed" class="graf graf--li graf-after--p">Ã¢ngulo da face;</li><li name="5a20" id="5a20" class="graf graf--li graf-after--li">pontos de interesseâ€Šâ€”â€Šolhos(o olho esquerdo Ã© distinguido do direito), boca, nariz, etc.</li><li name="3803" id="3803" class="graf graf--li graf-after--li">caracterÃ­sticasâ€Šâ€”â€Šdeteta se a pessoa estÃ¡ a sorrir, estÃ¡ com os olhos abertos, etc.</li></ul><p name="4503" id="4503" class="graf graf--p graf-after--li">E vocÃª jÃ¡ conhece o processo:</p><pre name="3b69" id="3b69" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">FirebaseVisionFaceDetector detector = FirebaseVision.getInstance()<br>    .getVisionFaceDetector();</code></pre><p name="5eb0" id="5eb0" class="graf graf--p graf-after--pre">Para esta experiÃªncia eu utilizei os valores que vÃªm definidos no exemplo dado na documentaÃ§Ã£o do Firebase. Mas provavelmente irei escrever um artigo com mais detalhes sobre a deteÃ§Ã£o facial.</p><h4 name="8969" id="8969" class="graf graf--h4 graf-after--p">Legendas paraÂ imagens</h4><p name="1a02" id="1a02" class="graf graf--p graf-after--h4">Esta funcionalidade reconhece entidades (legendas/rÃ³tulos) numa imagem. Por exemplo, na imagem abaixo, ela pode reconhecer as legendas: estÃ¡dio, desporto, evento, lazer, futebol, rede, planta,etc.</p><figure name="3199" id="3199" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 463px;"><div class="aspectRatioPlaceholder-fill"></div><img class="graf-image" data-image-id="1*78RCU2PiVq5Efkd1yGGxSw.jpeg" data-width="1024" data-height="678" src="https://cdn-images-1.medium.com/max/800/1*78RCU2PiVq5Efkd1yGGxSw.jpeg"></div></figure><p name="ccbd" id="ccbd" class="graf graf--p graf-after--figure">O ML Kit Ã© capaz de reconhecer mais de 400 entidades, incluindo: pessoas(aglomerado de gente, selfie, sorriso), atividades (a danÃ§ar, a comer, a surfar), coisas(um carro, um piano, uma receita), animais(pÃ¡ssaros, gatos, cÃ£es), plantas(flores, frutas, vegetais) ou atÃ© locais (praias, lagos, montanhas).</p><p name="7ae5" id="7ae5" class="graf graf--p graf-after--p">Para comeÃ§ar, temos de adicionar mais uma dependÃªncia ao gradle:</p><pre name="29b7" id="29b7" class="graf graf--pre graf-after--p">implementation &#39;com.google.firebase:firebase-ml-vision-image-label-model:15.0.0&#39;</pre><p name="9f5f" id="9f5f" class="graf graf--p graf-after--pre">E o nosso detector Ã©:</p><pre name="f04c" id="f04c" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">FirebaseVisionLabelDetector detector = FirebaseVision.getInstance()<br>    .getVisionLabelDetector();</code></pre><p name="c47f" id="c47f" class="graf graf--p graf-after--pre">Criei uma Activity que usa todos os detetores em simultÃ¢neo para poder testÃ¡-los. Se vocÃª quiser experimentar tambÃ©m, pode encontrar a Activity <a href="https://gist.github.com/rosariopfernandes/603bb860f7b1ccd82759ba8ce36e52b2" data-href="https://gist.github.com/rosariopfernandes/603bb860f7b1ccd82759ba8ce36e52b2" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">neste gist</a>.</p><h3 name="b052" id="b052" class="graf graf--h3 graf-after--p">Custom Models</h3><p name="5349" id="5349" class="graf graf--p graf-after--h3">Quem jÃ¡ tem experiÃªncia com Machine Learning pode utilizar os seus prÃ³prios modelos em conjunto com o ML Kit. Crie os modelos utilizando o TensorFlow Lite e importe-os na sua aplicaÃ§Ã£o.</p><p name="4776" id="4776" class="graf graf--p graf-after--p graf--trailing">Por enquanto nÃ£o entrarei em detalhes sobre os custom models, mas se vocÃª estiver interessado/curioso, pode saber mais na <a href="https://firebase.google.com/docs/ml-kit/android/use-custom-models?hl=pt-pt" data-href="https://firebase.google.com/docs/ml-kit/android/use-custom-models?hl=pt-pt" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">documentaÃ§Ã£o</a>.</p></div></div></section><section name="5a2e" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="4b8f" id="4b8f" class="graf graf--p graf--leading">Era de se esperar que o Firebase estivesse a preparar uma super novidade para o Google I/O 2018. O ML Kit Ã© certamente bem vindo. De certeza que irÃ¡ ajudar vÃ¡rios desenvolvedores a criarem aplicaÃ§Ãµes melhores.</p><p name="8682" id="8682" class="graf graf--p graf-after--p">A facilidade de uso Ã© uma das principais vantagens. Apesar da componente Cloud ser paga, a componente grÃ¡tis (on-device) Ã© bastante poderosa e Ãºtil.</p><p name="08d8" id="08d8" class="graf graf--p graf-after--p graf--trailing">Eu sempre quis experimentar a Cloud Vision API, mas o facto dela exigir um cartÃ£o de crÃ©dito sempre me fez hesitar. Mas agora que existe o ML Kit, posso por em prÃ¡tica todas as ideias que tenho tido para projetos futuros. Irei partilhar algumas delas aqui no Medium, entÃ£o continue me seguindo para nÃ£o perdÃª-las.</p></div></div></section><section name="2f75" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="e2d1" id="e2d1" class="graf graf--p graf--leading graf--trailing">Caso tenha alguma dÃºvida ou sugestÃ£o, deixe abaixo nos comentÃ¡rios. Se vocÃª estiver tentando usar o ML Kit e teve um problema, <a href="https://pt.stackoverflow.com/questions/ask?tags=firebase" data-href="https://pt.stackoverflow.com/questions/ask?tags=firebase" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">coloque ele no StackOverflow</a>, explicando o que vocÃª fez e qual foi o erro que teve. De certeza que vocÃª obterÃ¡ ajuda de mim ou de alguÃ©m da comunidade. ğŸ™‚</p></div></div>
